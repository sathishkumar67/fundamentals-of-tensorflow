# -*- coding: utf-8 -*-
"""fundamentals of tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNHsldJwqs3JRXg3T0pQum_rEugmk6Po

# Fundamental concepts of tensors using tensorflow

We're going to conver:
1. intro to tensors
2. getting info from tensors
3. manipulating tensors
4. tensors and numpy
5. using @tf.function(a way to speed up your regular python function)
6. using gpu with tensorflow(or tpu)
7. exercises yourself!!

# Introduction to tensors
"""

# Import Tensorflow
import tensorflow as tf

# Checking the version of tensoflow
print(tf.__version__)

# Create tensors with tf.constant()
scalar = tf.constant(7)
scalar

# Check the number of dimensions of a tensor (ndim stands for number of dimensions)
scalar.ndim

# Create a vector
vector = tf.constant([10,10])
vector

# Check the dimension of our vector
vector.ndim

# Create a matrix (has more than 1 dimension)
matrix = tf.constant([[10,7],
                      [7,10]])
matrix

matrix.ndim

# Create another matrix
another_matrix = tf.constant([[10.7,8.4],
                              [3.,4.],
                              [5.,8.]], dtype = tf.float16) # specify the data type with dtype parameter
another_matrix                                              # by default it shows as float32 for floating numbers
                                                            # 32 and 16 are precision numbers lower the number easier or less memory to store the tensors in our computer

# ndim for another matrix 
another_matrix.ndim

# total no of dimensions is the no of elements in the shape

# let's create a tensor
tensor = tf.constant(
    [[[1,2,3],
      [4,5,6]],
     [[7,8,9],
      [10,11,12]],
     [[13,14,15],
      [15,16,17]]]
)
tensor

tensor.ndim

sample_tensor = tf.constant([1,2,3,4],dtype = tf.int16)
sample_tensor

"""To Create a tensor:

* tf.constant() is used
* the data can be of any type eg: int...
there are 16 and 32 bit precision for the datatype based on the number the memory allocation of the tensor is done lower the number less memory for allocation.
* by default precision would be 32
* we can manually change the precision by
while creating a tensor eg:tensor = tf.constant([1,2,3],dtype = int16) it overrides the default precision and sets it as 16 bit precision

What we've created so far:
* Scalar: a single number
* Vector: a number with direction (eg: wind speed and direction)
* Matrix: a dimensional array of numbers
* Tensor: an n-dimensional array of number(when n can be any number eg:0,1,2,3...)
* ndim = no of elements in the shape of the tensor
* shape tells us the how many dimensions are there and how many elements are there

### Creating tensors with tf.variable
"""

# create the same tensor with tf.variable() as above
changeable_tensor = tf.Variable([10,7])
unchangeable_tensor = tf.constant([10,7])
changeable_tensor, unchangeable_tensor

# let's try to change one of the elements in our changeable tensor
changeable_tensor[0]

"""* we can access each elements of a tensor by accessing their index positions"""

changeable_tensor[0] = 7

# how about we try .assign()
changeable_tensor[0].assign(7)

# let's try to change our unchangeable tensor
unchangeable_tensor[0]

unchangeable_tensor[0] = 7

unchangeable_tensor[0].assign(7)

"""* tf.variable lets you to change the elements of the tensor 
* tf.constant is a constant tensor we can't change the elements in the tensor
* we can access the each elements by accessing their index position

## Creating Random Tensors

* random tensors are tensors of some abitrary size which contain random numbers.
* There are many ways to create a random tensor eg.normal, uniform... but the distribution of these no varies.
* when a nueral network starts to learn it starts with the random tensors. After learning for sometime it updates its representation(patterns, features, weights)

#### Normal Distribution
outputs the numbers from normal distribution
"""

# Create two random (but the same) tensors 
random_1 = tf.random.Generator.from_seed(42) # set seed for reproducibility (same as randomseed from numpy)
random_1 = random_1.normal(shape = (3,2))
random_2 = tf.random.Generator.from_seed(42)
random_2 =random_2.normal(shape = (3,2))
random_1, random_2, random_1 == random_2

"""#### Uniform Distribution
outputs numbers from uniform distribution
"""

uniform_distribution = tf.random.Generator.from_seed(42)
uniform_distribution = uniform_distribution.uniform(shape = (3,2))
uniform_distribution  # tf.random.uniform(seed = 42, shape = (3,2)) another way to create a random tensor

"""### Shuffle the order of elements in a tensor
this is used to shuffle the dataset so that the model can learn diff types of data
"""

# shuffle the tensors 
random_tensor = tf.random.uniform(seed = 42, shape = (3,2))
random_tensor

tf.random.shuffle(random_tensor)

tf.random.shuffle(random_tensor, seed = 42) # it won' produce the same results

# produces same results
tf.random.set_seed(42) # global seed
tf.random.shuffle(random_tensor, seed = 42) # operation-level seeds

"""Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.

Its interactions with operation-level seeds is as follows:

* If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.
* If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both global and operation-level seeds explicitly.
* If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence.
* If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.

inshorter:
* global seed is used when we want to reproduce same results in shuffle
* operation seed is used when we want to reproduce same results in picking random sequence
* operation and global seed can also be used in picking a random sequence
* when operation seed alone used in shuffling the result gets varied

### Other ways to make tensors
"""

# create a tensor of all ones
tf.ones([10, 7])

# create a tensor of all zeroes
tf.zeros(shape = (10,7)) # we can also use shape parameter

"""### creating tensors from numpy arrays"""

import numpy as np
numpy_a = np.arange(1,25, dtype = np.int32)
numpy_a

a = tf.constant(numpy_a)
b = tf.constant(numpy_a, shape = (3,8)) # shape must match the total no of elements
a, b

tf.ones(shape = (10,10))

"""what we covered:
* there are many ways to create tensors but there is also builtin method in tf to create tensor
* tf.zeros creates a tensor with zeroes with the given shape
* we can also create tensors from numpy array by passing those into tf.constant we can create tensors
* we can manualy change the shape of the tensors when passing the numpy arrays
* numpy arrays and tensorflow tensors will look like same but tensorflow tensor will able to run faster in gpu

## Getting information from tensors
these attributes are used when we are dealing with large tensors
* shape 
* rank(no of dimensions)
* axis or dimension (indexing)
* size
"""

sample_tensor = tf.zeros(shape = (2,3,4,5))
sample_tensor

# get various attributes of our tensor
print("datatype of every element:", sample_tensor.dtype)
print("number of dimensions (rank):", sample_tensor.ndim)
print("shape of tensor:", sample_tensor.shape)
print("elements along the 0 the axis:", sample_tensor.shape[0])
print("total number of elements in our tensor:", tf.size(sample_tensor))

# to print just the number
print("total number of elements in our tensor:", tf.size(sample_tensor).numpy())

"""### Indexing tensors 
tensors can be indexed just like python lists
* python lists are 1 dimension so we use only [:1] 
* but the tensors have more dimension so we have to apply to each and every one of them if we need to
"""

sample_tensor[:2,:2,:2,:2]

# get the first element from each dimension from each index except for the final one
sample_tensor[:1,:1,:1,:]

# we can [:] this if we want to print the whole thing using slicing

sample_tensor_2 = tf.constant([[1,2],
                               [3,4]])
sample_tensor_2,sample_tensor_2[-1]

# add in extra dimension to our tensor
sample_tensor_3 = sample_tensor_2[..., tf.newaxis] # adds the new axis at the end
sample_tensor_3                                    # (...) copies the current dimension and adds an extra dimension

# alternative to tf.newaxis
tf.expand_dims(sample_tensor_2,axis = -1)

# axis specifies where you would want to expand the dimension
# if 0 is specified new axis will be added in the begining

"""## manipulating tensors(tensor operations)
* manipulating tensors with the basic operations is a little bit slower in gpu
* using tensor builtin method for manipulating tensors are lot better in gpu
* these are element vise operations (ie) go to one element at a time
"""

# you can change the values of a tensor using basic operators
tensor = tf.ones(shape = (2,2))
tensor

tensor+10

tensor * 10

tensor - 10

tensor/9

tf.multiply(tensor,10)

tf.add(tensor,200)

"""## matrix multiplication

2 rules for matrix multiplication

1. the inner dimensions must match
2. the resulting matrix has the shape of the outer matrix
"""

# matrix multiplication in tensorflow
print(tensor)

tensor = tf.constant([[10,7],
                     [3,4]])
tensor

# we can remove the middle part in the function
tf.matmul(tensor,tensor)

# it's an element wise multiplication
tensor * tensor

# matrix multiplication with python operator "@"
tensor @ tensor

tensor.shape

# create a tensor (3,2)
tensor2 = tf.constant([[1,2,3],
                       [4,5,6]])
tensor2

tensor3 = tf.constant([[1,2],
                       [2,4],
                       [5,4]])
tensor3

tensor2 @ tensor3

tf.matmul(tensor2, tensor3)

tf.matmul(tensor3, tensor3)

tensor3

# changing the shape of the tensor(shuffles the no and changes the shape based on our requirement)
tensor_3 = tf.reshape(tensor3,shape = (2,3))
tensor_3

tensor3 @ tensor_3

tensor3.shape, tensor_3.shape

tf.matmul(tensor3,tensor_3)

# transpose matrix(transposes axis)
tf.transpose (tensor3),tensor3

# try matrix multiplication with transpose rather than reshape
tf.matmul(tf.transpose(tensor3),tensor3)

"""## dot product
matrix multiplication is also referred to as the dot product.

you can perform matrimultiplication using:
1. tf.matmul
2. tensor @ tensor@
3. tf.tensordot()
"""

# perform the dot product on tensor3
tf.tensordot(tf.transpose(tensor3),tensor3,axes = 1)

tf.matmul(tensor3,tf.reshape(tensor3,shape = (2,3)) )

tf.matmul(tf.reshape(tensor3,shape = (2,3)),tensor3)

tf.matmul(tf.transpose(tensor3),tensor3)

tensor3

tf.transpose(tensor3)

tf.reshape(tensor3,shape = (2,3))

tf.__version__

"""### changing the datatype of a tensor
default is 32 bit precision
* 16 bit precision is a lot faster
"""

# create a new tensor with default datatype
B = tf.constant([1.4,5.6])
B

B.dtype

C = tf.constant([1,2])
C

C.dtype

# change the float32 to float16 (reduced precision)
B = tf.cast(B,dtype = tf.float16)
B.dtype

C = tf.cast(C,dtype = tf.int16)
C.dtype

# change from int16 to float16
E = tf.cast(C,dtype = tf.float16)
E,E.dtype

"""### aggregating tensors

condensing them from multiple values down to a smaller amount of values
"""

D = tf.constant([-7,-10])
D

# get the absolute values
tf.abs(D)

# create a random tensor with values between 0 and 100 of size 50
E = tf.constant(np.random.randint(0,100,size = 50))
E

tf.size(E), E.shape,E.ndim

# find the min
tf.reduce_min(E)

# find the maximum 
tf.reduce_max(E)

# find the mean
tf.reduce_mean(E)

# sum of the tensor
tf.reduce_sum(E)

# find the variance of our tensor, we need access to tensorflow_probability
import tensorflow_probability as tfp
tfp.stats.variance(E)

# find the standard deviation
tf.math.reduce_std(tf.cast(E, dtype = tf.float32))

"""* if we get input must be real or complex error then we should convert our dtype to float"""

# find the variance of our tensor(for other methods we may not need to use math but for variance and stddev we must use math)
tf.math.reduce_variance(tf.cast(E,dtype = tf.float32))

"""* representation outputs are prediction probabilities

### find the positional maximum and minimum
"""

# create a new tensor for finding positional minimum and maximum
tf.random.set_seed(42)
F = tf.random.uniform(shape = [50])
F

# find the positional max
tf.argmax(F)

F[42]

F[tf.argmax(F)]

# find the max value of F
tf.reduce_max(F)

# check of equality
F[tf.argmax(F)] == tf.reduce_max(F)

# find the position min
tf.argmin(F)

F[tf.argmin(F)]

F[tf.argmin(F)] == tf.reduce_min(F)

"""### squeezing a tensor(removing all single dimensions)
squeeze is used when there is a lot of single dimension in your tensor and you need to remove that dimension
"""

# create a tensor to get  started 
G = tf.constant(tf.random.uniform(shape = [50]),shape = (1,1,1,1,50))
G

G.shape

G_squeeze = tf.squeeze(G)
G_squeeze,G_squeeze.shape

"""### one hot encoding 
* if we receive any value error then we should probably change the datatype of the input from int to float
* most builtin fucntions requries us to pass the inputs as float type
* we can't send strings to the neural networks we should convert to numbers that's why one hot encoding is used 
* it bascially creates an array with the help of somelist
"""

# create a list of indices
some_list = [0,1,2,3] # could be red, green, blue, purple

# one hot encode our list of indices
tf.one_hot(some_list,depth = 4) # here length of the list and depth (ie) 4 * 4
                                # here depth represents how many elements should be added per array

# changing the datatype of the one hot encoder
tf.cast(tf.one_hot(some_list, depth = 4),dtype = tf.int32)

# setting a values to the encoder
tf.one_hot(some_list,depth = 4, on_value="it's not over", off_value = "it's over")

### squaring, log, sqroot
B = tf.range(1,10)
B

tf.square(B)

# find the squareroot
tf.math.sqrt(tf.cast(B,dtype = tf.float32)) # requires float32 datatype for squareroot

tf.math.log(tf.cast(B,dtype = tf.float32))

"""### tensors and numpy
tensorflow interacts beautifully with numpy arrays
"""

# create a tensor directly from a numpy array
J = tf.constant(np.array([1.4,5.5]))
J

# convert our tensor back to numpy array
np.array(J),type(np.array(J))

# another way to convert our tensor to numpy
J.numpy(),type(J.numpy())

# the default types of each are different
numpy_j = tf.constant(np.array([4.5]))
tensor_j = tf.constant([5.5])

# check the datatypes of each
numpy_j, tensor_j

"""* the datatype might be different when we comparing between numpy and tensor

### finding access to gpu
"""

# returns a list of physical devices available
tf.config.list_physical_devices()

!nvidia-smi

